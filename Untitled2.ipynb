{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auPkcPukxR4l",
        "outputId": "387d6459-ddf1-43eb-d90a-8d066da2825c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            " UNIVERSAL WIKIPEDIA CONTENT GENERATOR\n",
            "ðŸŽ¯ ENTER ANY TOPIC\n",
            "=====================\n",
            "Test with:\n",
            "â€¢ Machine Learning\n",
            "â€¢ Artificial Intelligence\n",
            "â€¢ Quantum Computing\n",
            "â€¢ Blockchain\n",
            "â€¢ Renewable Energy\n",
            "=====================\n",
            "\n",
            " Enter topic (or 'quit'): Machine Learning\n",
            "ðŸš€ Generating: Machine Learning\n",
            " Researching: Machine Learning\n",
            " Using special method for Machine Learning...\n",
            " Generated 1137 words\n",
            "\n",
            " CONTENT: Machine Learning\n",
            "============================================================\n",
            "# Machine Learning\n",
            "## Overview\n",
            "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n",
            "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics.\n",
            "Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.\n",
            "From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework.\n",
            "== History ==\n",
            "The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period.\n",
            "## Details\n",
            "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. Most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework. \n",
            "## History\n",
            "\n",
            "... [full content in saved file] ...\n",
            "============================================================\n",
            " Sources: 3\n",
            "\n",
            " Pages:\n",
            "   1. Machine learning\n",
            "   2. Neural network (machine learning)\n",
            "   3. Attention (machine learning)\n",
            "\n",
            "==================================================\n",
            " Another topic? (y/n): y\n",
            "\n",
            " Enter topic (or 'quit'): Deep Learning\n",
            "ðŸš€ Generating: Deep Learning\n",
            " Researching: Deep Learning\n",
            " Found 3 sources\n",
            " Generated 1099 words\n",
            "\n",
            " CONTENT: Deep Learning\n",
            "============================================================\n",
            "# Deep Learning\n",
            "## Overview\n",
            "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised.\n",
            "Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
            "Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.\n",
            "== Overview ==\n",
            "Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\n",
            "Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation.\n",
            "## Details\n",
            "In machine learning, deep learning focuses on utilizing multilayered neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be supervised, semi-supervised or unsupervised. Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance. Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose. \n",
            "## Overview\n",
            "Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a progressively more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction. The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are ab \n",
            "\n",
            "... [full content in saved file] ...\n",
            "============================================================\n",
            " Sources: 3\n",
            "\n",
            " Pages:\n",
            "   1. Deep learning\n",
            "   2. Transformer (deep learning architecture)\n",
            "   3. Deep reinforcement learning\n",
            "\n",
            "==================================================\n",
            " Another topic? (y/n): n\n",
            " Thank you!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q wikipedia requests\n",
        "\n",
        "import os\n",
        "import json\n",
        "import wikipedia\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "print(\" UNIVERSAL WIKIPEDIA CONTENT GENERATOR\")\n",
        "\n",
        "# Setup\n",
        "os.makedirs('/content/outputs', exist_ok=True)\n",
        "wikipedia.set_lang(\"en\")\n",
        "\n",
        "# ------------------ UNIVERSAL RESEARCH AGENT ------------------\n",
        "class UniversalResearchAgent:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def research(self, topic: str) -> List[Dict]:\n",
        "        print(f\" Researching: {topic}\")\n",
        "        sources = []\n",
        "\n",
        "        # Special handling for \"Machine Learning\"\n",
        "        if topic.lower() == \"machine learning\":\n",
        "            return self._research_machine_learning()\n",
        "\n",
        "        try:\n",
        "            # Method 1: Try Wikipedia library\n",
        "            try:\n",
        "                main_page = wikipedia.page(topic, auto_suggest=False)\n",
        "                main_summary = wikipedia.summary(topic, sentences=12, auto_suggest=False)\n",
        "                sources.append({\n",
        "                    \"title\": main_page.title,\n",
        "                    \"url\": main_page.url,\n",
        "                    \"summary\": main_summary,\n",
        "                    \"content\": main_page.content[:4000],\n",
        "                    \"method\": \"wikipedia_library\"\n",
        "                })\n",
        "\n",
        "            except wikipedia.exceptions.DisambiguationError as e:\n",
        "                # Use first option from disambiguation\n",
        "                first_option = e.options[0]\n",
        "                main_page = wikipedia.page(first_option, auto_suggest=False)\n",
        "                main_summary = wikipedia.summary(first_option, sentences=12, auto_suggest=False)\n",
        "                sources.append({\n",
        "                    \"title\": main_page.title,\n",
        "                    \"url\": main_page.url,\n",
        "                    \"summary\": main_summary,\n",
        "                    \"content\": main_page.content[:4000],\n",
        "                    \"method\": \"disambiguation\"\n",
        "                })\n",
        "\n",
        "            except wikipedia.exceptions.PageError:\n",
        "                # Method 2: Use Wikipedia API directly\n",
        "                print(\" Using Wikipedia API directly...\")\n",
        "                api_sources = self._wikipedia_api_search(topic)\n",
        "                if api_sources:\n",
        "                    sources.extend(api_sources)\n",
        "                else:\n",
        "                    print(\" No Wikipedia content found\")\n",
        "                    return []\n",
        "\n",
        "            # Get related topics\n",
        "            related_sources = self._get_related_topics(topic)\n",
        "            sources.extend(related_sources)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Research error: {e}\")\n",
        "            # Final fallback: API search\n",
        "            api_sources = self._wikipedia_api_search(topic)\n",
        "            if api_sources:\n",
        "                sources.extend(api_sources)\n",
        "\n",
        "        print(f\" Found {len(sources)} sources\")\n",
        "        return sources\n",
        "\n",
        "    def _research_machine_learning(self) -> List[Dict]:\n",
        "        \"\"\"Special research for Machine Learning\"\"\"\n",
        "        print(\" Using special method for Machine Learning...\")\n",
        "        sources = []\n",
        "\n",
        "        try:\n",
        "            # Try different page names for Machine Learning\n",
        "            possible_names = [\n",
        "                \"Machine learning\",\n",
        "                \"ML (machine learning)\",\n",
        "                \"Machine Learning (field)\"\n",
        "            ]\n",
        "\n",
        "            for name in possible_names:\n",
        "                try:\n",
        "                    page = wikipedia.page(name, auto_suggest=False)\n",
        "                    summary = wikipedia.summary(name, sentences=15, auto_suggest=False)\n",
        "\n",
        "                    sources.append({\n",
        "                        \"title\": page.title,\n",
        "                        \"url\": page.url,\n",
        "                        \"summary\": summary,\n",
        "                        \"content\": page.content[:5000],\n",
        "                        \"method\": \"special_handling\"\n",
        "                    })\n",
        "\n",
        "                    # Get related topics\n",
        "                    related = self._get_related_topics(\"machine learning\")\n",
        "                    sources.extend(related)\n",
        "                    break\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if not sources:\n",
        "                # Fallback to API\n",
        "                api_sources = self._wikipedia_api_search(\"machine learning\")\n",
        "                if api_sources:\n",
        "                    sources.extend(api_sources)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Machine Learning research failed: {e}\")\n",
        "\n",
        "        return sources\n",
        "\n",
        "    def _wikipedia_api_search(self, topic: str) -> List[Dict]:\n",
        "        \"\"\"Use Wikipedia API directly as fallback\"\"\"\n",
        "        try:\n",
        "            url = \"https://en.wikipedia.org/w/api.php\"\n",
        "            params = {\n",
        "                \"action\": \"query\",\n",
        "                \"list\": \"search\",\n",
        "                \"srsearch\": topic,\n",
        "                \"format\": \"json\",\n",
        "                \"srlimit\": 5\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            data = response.json()\n",
        "\n",
        "            sources = []\n",
        "            search_results = data.get(\"query\", {}).get(\"search\", [])\n",
        "\n",
        "            for result in search_results[:3]:\n",
        "                title = result[\"title\"]\n",
        "                page_url = f\"https://en.wikipedia.org/wiki/{quote_plus(title)}\"\n",
        "\n",
        "                # Get summary using API\n",
        "                summary_params = {\n",
        "                    \"action\": \"query\",\n",
        "                    \"prop\": \"extracts\",\n",
        "                    \"exintro\": True,\n",
        "                    \"explaintext\": True,\n",
        "                    \"titles\": title,\n",
        "                    \"format\": \"json\"\n",
        "                }\n",
        "\n",
        "                summary_response = requests.get(url, params=summary_params, timeout=10)\n",
        "                summary_data = summary_response.json()\n",
        "                pages = summary_data.get(\"query\", {}).get(\"pages\", {})\n",
        "\n",
        "                summary = \"\"\n",
        "                for page_id, page_data in pages.items():\n",
        "                    summary = page_data.get(\"extract\", \"\")[:500] + \"...\"\n",
        "                    break\n",
        "\n",
        "                sources.append({\n",
        "                    \"title\": title,\n",
        "                    \"url\": page_url,\n",
        "                    \"summary\": summary if summary else f\"Information about {title}\",\n",
        "                    \"method\": \"api_fallback\"\n",
        "                })\n",
        "\n",
        "            return sources\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" API search failed: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _get_related_topics(self, topic: str) -> List[Dict]:\n",
        "        \"\"\"Get related topics\"\"\"\n",
        "        related_sources = []\n",
        "        try:\n",
        "            search_results = wikipedia.search(topic, results=4)\n",
        "            for result in search_results:\n",
        "                if result.lower() != topic.lower():\n",
        "                    try:\n",
        "                        page = wikipedia.page(result, auto_suggest=False)\n",
        "                        summary = wikipedia.summary(result, sentences=4, auto_suggest=False)\n",
        "                        related_sources.append({\n",
        "                            \"title\": page.title,\n",
        "                            \"url\": page.url,\n",
        "                            \"summary\": summary,\n",
        "                            \"is_related\": True\n",
        "                        })\n",
        "                    except:\n",
        "                        continue\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return related_sources[:2]  # Return max 2 related topics\n",
        "\n",
        "# ------------------ CONTENT ORGANIZER ------------------\n",
        "class ContentOrganizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def create_article(self, topic: str, sources: List[Dict]) -> str:\n",
        "        \"\"\"Create article from research\"\"\"\n",
        "        if not sources:\n",
        "            return self._create_no_content_message(topic)\n",
        "\n",
        "        article_parts = []\n",
        "\n",
        "        # Title\n",
        "        article_parts.append(f\"# {topic}\\n\")\n",
        "\n",
        "        # Main content from first source\n",
        "        main_source = sources[0]\n",
        "        if 'summary' in main_source:\n",
        "            article_parts.append(\"## Overview\\n\")\n",
        "            article_parts.append(main_source['summary'])\n",
        "            article_parts.append(\"\")\n",
        "\n",
        "        # Extract sections from content if available\n",
        "        if 'content' in main_source:\n",
        "            sections = self._extract_sections(main_source['content'])\n",
        "            for section in sections:\n",
        "                article_parts.append(f\"## {section['title']}\\n\")\n",
        "                article_parts.append(section['content'])\n",
        "                article_parts.append(\"\")\n",
        "\n",
        "        # Related topics\n",
        "        related_sources = [s for s in sources if s.get('is_related')]\n",
        "        if related_sources:\n",
        "            article_parts.append(\"## Related Topics\\n\")\n",
        "            for source in related_sources:\n",
        "                article_parts.append(f\"### {source['title']}\\n\")\n",
        "                article_parts.append(source['summary'])\n",
        "                article_parts.append(\"\")\n",
        "\n",
        "        # References\n",
        "        article_parts.append(\"## References\\n\")\n",
        "        for i, source in enumerate(sources, 1):\n",
        "            article_parts.append(f\"[{i}] {source['title']}\")\n",
        "            article_parts.append(f\"    {source['url']}\")\n",
        "            if 'method' in source:\n",
        "                article_parts.append(f\"    Source: {source['method']}\")\n",
        "            article_parts.append(\"\")\n",
        "\n",
        "        return \"\\n\".join(article_parts)\n",
        "\n",
        "    def _extract_sections(self, content: str) -> List[Dict]:\n",
        "        \"\"\"Extract sections from content\"\"\"\n",
        "        sections = []\n",
        "        lines = content.split('\\n')\n",
        "        current_section = {\"title\": \"Details\", \"content\": \"\"}\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line.startswith('==') and line.endswith('=='):\n",
        "                if current_section[\"content\"].strip():\n",
        "                    sections.append(current_section.copy())\n",
        "                current_section = {\n",
        "                    \"title\": line.replace('=', '').strip(),\n",
        "                    \"content\": \"\"\n",
        "                }\n",
        "            elif line and len(line) > 20 and not line.startswith(('{', '[', '|')):\n",
        "                current_section[\"content\"] += line + \" \"\n",
        "\n",
        "        if current_section[\"content\"].strip():\n",
        "            sections.append(current_section)\n",
        "\n",
        "        return sections[:6]\n",
        "\n",
        "    def _create_no_content_message(self, topic: str) -> str:\n",
        "        return f\"# {topic}\\n\\nNo Wikipedia content could be retrieved for this topic.\"\n",
        "\n",
        "# ------------------ MAIN EXECUTION ------------------\n",
        "def generate_content(topic: str):\n",
        "    \"\"\"Generate content for any topic\"\"\"\n",
        "    print(f\"ðŸš€ Generating: {topic}\")\n",
        "\n",
        "    researcher = UniversalResearchAgent()\n",
        "    organizer = ContentOrganizer()\n",
        "\n",
        "    sources = researcher.research(topic)\n",
        "    article = organizer.create_article(topic, sources)\n",
        "\n",
        "    # Save\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"/content/outputs/{topic.replace(' ', '_').lower()}.txt\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"TOPIC: {topic}\\n\")\n",
        "        f.write(f\"SOURCES: {len(sources)}\\n\")\n",
        "        f.write(f\"GENERATED: {datetime.now().isoformat()}\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "        f.write(article)\n",
        "\n",
        "    word_count = len(article.split())\n",
        "    print(f\" Generated {word_count} words\")\n",
        "    return article, sources, filename\n",
        "\n",
        "def display_content(article: str, topic: str):\n",
        "    \"\"\"Display content\"\"\"\n",
        "    print(f\"\\n CONTENT: {topic}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    lines = article.split('\\n')\n",
        "    for line in lines[:20]:\n",
        "        if line.strip():\n",
        "            print(line)\n",
        "\n",
        "    if len(lines) > 20:\n",
        "        print(\"\\n... [full content in saved file] ...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# ------------------ RUN ------------------\n",
        "print(\"ðŸŽ¯ ENTER ANY TOPIC\")\n",
        "print(\"=====================\")\n",
        "print(\"Test with:\")\n",
        "print(\"â€¢ Machine Learning\")\n",
        "print(\"â€¢ Artificial Intelligence\")\n",
        "print(\"â€¢ Quantum Computing\")\n",
        "print(\"â€¢ Blockchain\")\n",
        "print(\"â€¢ Renewable Energy\")\n",
        "print(\"=====================\")\n",
        "\n",
        "while True:\n",
        "    topic = input(\"\\n Enter topic (or 'quit'): \").strip()\n",
        "\n",
        "    if topic.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    if not topic:\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        article, sources, filename = generate_content(topic)\n",
        "\n",
        "        if sources:\n",
        "            display_content(article, topic)\n",
        "            print(f\" Sources: {len(sources)}\")\n",
        "            print(\"\\n Pages:\")\n",
        "            for i, source in enumerate(sources, 1):\n",
        "                print(f\"   {i}. {source['title']}\")\n",
        "        else:\n",
        "            print(\" No content found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error: {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    cont = input(\" Another topic? (y/n): \").strip().lower()\n",
        "    if cont != 'y':\n",
        "        break\n",
        "\n",
        "print(\" Thank you!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d9S4eFV9xTE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}